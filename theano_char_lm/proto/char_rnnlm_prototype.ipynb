{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a character level Fuel dataset from text file\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from toolz import merge\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from toolz import merge\n",
    "import pandas as pd\n",
    "from six import add_metaclass\n",
    "from picklable_itertools.extras import equizip\n",
    "from blocks.bricks.recurrent import (GatedRecurrent, Bidirectional)\n",
    "from blocks.initialization import IsotropicGaussian, Constant, Orthogonal\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from blocks.bricks import Initializable\n",
    "from blocks.bricks.base import application, Brick, lazy\n",
    "from blocks.bricks import Tanh, Linear, MLP\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.utils import (shared_floatx_nans, dict_union)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "\n",
    "\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to your training data\n",
    "TRAINING_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.train.txt'\n",
    "DEV_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.dev.txt'\n",
    "\n",
    "UNKNOWN_TOKEN = '|'\n",
    "EOS_TOKEN = '`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    all_symbols = set()\n",
    "    with codecs.open(dataset, encoding='utf8') as inp: \n",
    "        for l in inp.read().strip().split('\\n'):\n",
    "            all_symbols.update(l)\n",
    "    return all_symbols\n",
    "    \n",
    "    \n",
    "def get_y_file(dataset):\n",
    "    \n",
    "    # a file to hold the Y representation of the dataset\n",
    "    y_tmp_file = os.path.join(os.path.basename(dataset), dataset+'_temp_y.txt')\n",
    "    \n",
    "    with codecs.open(dataset, encoding='utf8') as inp:\n",
    "        with codecs.open(\n",
    "            y_tmp_file, 'wb',encoding='utf8') as y_out:\n",
    "            for l in inp.read().strip().split('\\n'):\n",
    "                y_seq = l[1:] + EOS_TOKEN\n",
    "                assert len(l) == len(y_seq)\n",
    "\n",
    "                y_out.write(''.join(y_seq) + '\\n')\n",
    "\n",
    "    return y_tmp_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add an unknown token in case we observe a new character at prediction time\n",
    "vocab = get_vocab(TRAINING_DATASET)\n",
    "vocab.update([UNKNOWN_TOKEN, EOS_TOKEN])\n",
    "word2idx = {v:k for k,v in enumerate(vocab)}\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "\n",
    "# axis swapping so that we get (time, batch, features)\n",
    "def swapaxes(data):\n",
    "    \"\"\"Switch the axes in the sequence parts of the data tuple\"\"\"\n",
    "    return tuple(array.swapaxes(0,1) for array in data)\n",
    "\n",
    "class _too_long(object):\n",
    "    \"\"\"Filters sequences longer than given sequence length.\"\"\"\n",
    "    def __init__(self, seq_len=50):\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __call__(self, sentence_pair):\n",
    "        return all([len(sentence) <= self.seq_len\n",
    "                    for sentence in sentence_pair])\n",
    "\n",
    "\n",
    "def get_stream(vocab, x_file,\n",
    "               seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "    \"\"\"Prepares the data stream.\"\"\"\n",
    "    \n",
    "    y_file = get_y_file(x_file)\n",
    "\n",
    "    def _length(item):\n",
    "        \"\"\"Item is assumed to be (x,y)\"\"\"\n",
    "        return len(item[0])\n",
    "    \n",
    "    # Get text files from both source and target\n",
    "    X = TextFile([x_file], vocab, bos_token=None, eos_token=None,\n",
    "                   unk_token=unk_token, level='character')\n",
    "\n",
    "    Y = TextFile([y_file], vocab, bos_token=None, eos_token=None,\n",
    "                       unk_token=unk_token, level='character')\n",
    "\n",
    "    # Merge them to get x1, x2 pairs\n",
    "    stream = Merge([X.get_example_stream(),\n",
    "                    Y.get_example_stream()],\n",
    "                    ('x', 'y'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream, predicate=_too_long(seq_len=seq_len))\n",
    "\n",
    "    # LOOKAHEAD SORT\n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "    stream = Batch(stream,\n",
    "                   iteration_scheme=ConstantScheme(\n",
    "                   batch_size*sort_k_batches))\n",
    "    \n",
    "    # Sort all samples in the read-ahead batch\n",
    "    stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "    stream = Unpack(stream)\n",
    "    # END LOOKAHEAD SORT\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    # Pad sequences that are short\n",
    "    masked_stream = Padding(stream, mask_sources=['x1', 'x2'])\n",
    "    \n",
    "    # transpose tensors to get (time, batch, features)\n",
    "    masked_stream = Mapping(masked_stream, swapaxes)\n",
    "\n",
    "    return masked_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_stream(vocab, x_file, y_file,\n",
    "#                seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "\n",
    "train_stream = get_stream(word2idx, TRAINING_DATASET)\n",
    "\n",
    "dev_stream = get_stream(word2idx, DEV_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "\n",
    "class RNNLM(Initializable):\n",
    "    \"\"\"Model for character level LM\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, state_dim, output_dim, **kwargs):\n",
    "        super(RNNLM, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lookup = LookupTable(name='embeddings')\n",
    "        self.transition = GatedRecurrent(activation=Tanh(), dim=state_dim)\n",
    "        \n",
    "        self.fork = Fork(\n",
    "            [name for name in self.transition.apply.sequences\n",
    "             if name != 'mask'], prototype=Linear(), name='fork')\n",
    "\n",
    "        # output layer -- this may need to be changed\n",
    "        self.output_layer = Linear(name='output_layer')\n",
    "        \n",
    "        self.children = [self.lookup, self.transition,\n",
    "                         self.fork, self.output_layer]\n",
    "\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.lookup.length = self.vocab_size\n",
    "        self.lookup.dim = self.embedding_dim\n",
    "\n",
    "        self.fork.input_dim = self.embedding_dim\n",
    "        self.fork.output_dims = [self.transition.get_dim(name)\n",
    "                                 for name in self.fork.output_names]\n",
    "        \n",
    "        self.output_layer.input_dim = self.state_dim\n",
    "        self.output_layer.output_dim = self.output_dim\n",
    "\n",
    "    def cost(self, x, x_mask, y, y_mask):\n",
    "        \n",
    "        representation = self.lookup.apply(x)\n",
    "        \n",
    "        states = self.transition.apply(**merge(self.fork.apply(representation, as_dict=True), {'mask': x_mask}))\n",
    "        \n",
    "        # get cost from output layer, transform inputs as necessary\n",
    "        \n",
    "        \n",
    "        return states\n",
    "#         return cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "STATE_DIM=10\n",
    "EMBEDDING_DIM=4\n",
    "OUTPUT_DIM=3\n",
    "\n",
    "test_rnnlm = RNNLM(VOCAB_SIZE, EMBEDDING_DIM, STATE_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize for testing\n",
    "test_rnnlm.weights_init = IsotropicGaussian(0.1)\n",
    "test_rnnlm.biases_init = Constant(0)\n",
    "test_rnnlm.push_initialization_config()\n",
    "test_rnnlm.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_rnnlm.lookup.W.get_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnusedInputError",
     "evalue": "theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_mask.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnusedInputError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-38fc00e55725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_rnnlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtest_cost_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    318\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    524\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1777\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1778\u001b[0m             defaults)\n\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1415\u001b[0m         \u001b[1;31m# Check if some input variables are unused\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1416\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_unused_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1418\u001b[0m         \u001b[1;31m# Make a list of (SymbolicInput|SymblicInputKits, indices,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/programs/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m_check_unused_inputs\u001b[1;34m(self, inputs, outputs, on_unused_input)\u001b[0m\n\u001b[0;32m   1552\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mon_unused_input\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m                     raise UnusedInputError(msg % (inputs.index(i),\n\u001b[1;32m-> 1554\u001b[1;33m                                                   i.variable, err_msg))\n\u001b[0m\u001b[0;32m   1555\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1556\u001b[0m                     raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[1;31mUnusedInputError\u001b[0m: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: x_mask.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'."
     ]
    }
   ],
   "source": [
    "# Make symbolic variables\n",
    "x = T.lmatrix(\"x\")\n",
    "x_mask = T.matrix(\"x_mask\")\n",
    "y = T.lmatrix(\"y\")\n",
    "y_mask = T.matrix(\"y_mask\")\n",
    "\n",
    "# cost_given_input.name = 'cost_given_input'\n",
    "\n",
    "test_val = test_rnnlm.cost(x, x_mask, y, y_mask)\n",
    "\n",
    "test_cost_func = theano.function([x, x_mask, y, y_mask], test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
