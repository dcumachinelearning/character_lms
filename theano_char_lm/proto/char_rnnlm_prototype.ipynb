{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a character level Fuel dataset from text file\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from toolz import merge\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from toolz import merge\n",
    "import pandas as pd\n",
    "from six import add_metaclass\n",
    "from picklable_itertools.extras import equizip\n",
    "from blocks.bricks.recurrent import (GatedRecurrent, Bidirectional)\n",
    "from blocks.initialization import IsotropicGaussian, Constant, Orthogonal\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from blocks.bricks import Initializable\n",
    "from blocks.bricks.base import application, Brick, lazy\n",
    "from blocks.bricks import Tanh, Linear, MLP\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.utils import (shared_floatx_nans, dict_union)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "from blocks.bricks import Tanh, Linear, Softmax, MLP\n",
    "\n",
    "\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to your training data\n",
    "TRAINING_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.train.txt'\n",
    "DEV_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.dev.txt'\n",
    "\n",
    "UNKNOWN_TOKEN = '|'\n",
    "EOS_TOKEN = '`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    all_symbols = set()\n",
    "    with codecs.open(dataset, encoding='utf8') as inp: \n",
    "        for l in inp.read().strip().split('\\n'):\n",
    "            all_symbols.update(l)\n",
    "    return all_symbols\n",
    "    \n",
    "    \n",
    "def get_y_file(dataset):\n",
    "    \n",
    "    # a file to hold the Y representation of the dataset\n",
    "    y_tmp_file = os.path.join(os.path.basename(dataset), dataset+'_temp_y.txt')\n",
    "    \n",
    "    with codecs.open(dataset, encoding='utf8') as inp:\n",
    "        with codecs.open(\n",
    "            y_tmp_file, 'wb',encoding='utf8') as y_out:\n",
    "            for l in inp.read().strip().split('\\n'):\n",
    "                y_seq = l[1:] + EOS_TOKEN\n",
    "                assert len(l) == len(y_seq)\n",
    "    \n",
    "                y_out.write(''.join(y_seq) + '\\n')\n",
    "\n",
    "    return y_tmp_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharTextFile(TextFile):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CharTextFile, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    # override parent method to only do rstrip instead of strip\n",
    "    def get_data(self, state=None, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        sentence = next(state)\n",
    "        if self.preprocess is not None:\n",
    "            sentence = self.preprocess(sentence)\n",
    "        data = [self.dictionary[self.bos_token]] if self.bos_token else []\n",
    "        if self.level == 'word':\n",
    "            data.extend(self.dictionary.get(word,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for word in sentence.split())\n",
    "        else:\n",
    "            data.extend(self.dictionary.get(char,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for char in sentence.rstrip())\n",
    "        if self.eos_token:\n",
    "            data.append(self.dictionary[self.eos_token])\n",
    "        return (data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add an unknown token in case we observe a new character at prediction time\n",
    "vocab = get_vocab(TRAINING_DATASET)\n",
    "vocab.update([UNKNOWN_TOKEN, EOS_TOKEN])\n",
    "word2idx = {v:k for k,v in enumerate(vocab)}\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "\n",
    "# axis swapping so that we get (time, batch, features)\n",
    "def swapaxes(data):\n",
    "    \"\"\"Switch the axes in the sequence parts of the data tuple\"\"\"\n",
    "    return tuple(array.swapaxes(0,1) for array in data)\n",
    "\n",
    "class _too_long(object):\n",
    "    \"\"\"Filters sequences longer than given sequence length.\"\"\"\n",
    "    def __init__(self, seq_len=50):\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __call__(self, sentence_pair):\n",
    "        return all([len(sentence) <= self.seq_len\n",
    "                    for sentence in sentence_pair])\n",
    "\n",
    "\n",
    "def get_stream(vocab, x_file,\n",
    "               seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "    \"\"\"Prepares the data stream.\"\"\"\n",
    "    \n",
    "    y_file = get_y_file(x_file)\n",
    "\n",
    "    def _length(item):\n",
    "        \"\"\"Item is assumed to be (x,y)\"\"\"\n",
    "        return len(item[0])\n",
    "    \n",
    "    # Get text files from both source and target\n",
    "    X = CharTextFile([x_file], vocab, bos_token=None, eos_token=None,\n",
    "                 unk_token=unk_token, level='character')\n",
    "\n",
    "    Y = CharTextFile([y_file], vocab, bos_token=None, eos_token=None,\n",
    "                 unk_token=unk_token, level='character')\n",
    "\n",
    "    # Merge them to get x1, x2 pairs\n",
    "    stream = Merge([X.get_example_stream(),\n",
    "                    Y.get_example_stream()],\n",
    "                    ('x', 'y'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream, predicate=_too_long(seq_len=seq_len))\n",
    "\n",
    "    # LOOKAHEAD SORT\n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "#     stream = Batch(stream,\n",
    "#                    iteration_scheme=ConstantScheme(\n",
    "#                    batch_size*sort_k_batches))\n",
    "    \n",
    "    # Sort all samples in the read-ahead batch\n",
    "#     stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "#     stream = Unpack(stream)\n",
    "    # END LOOKAHEAD SORT\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    # Pad sequences that are short\n",
    "    masked_stream = Padding(stream)\n",
    "    \n",
    "    # transpose tensors to get (time, batch, features)\n",
    "    masked_stream = Mapping(masked_stream, swapaxes)\n",
    "\n",
    "    return masked_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_stream(vocab, x_file, y_file,\n",
    "#                seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "MAX_SEQ_LEN = 200\n",
    "\n",
    "train_stream = get_stream(word2idx, TRAINING_DATASET, batch_size=BATCH_SIZE, seq_len=MAX_SEQ_LEN)\n",
    "dev_stream = get_stream(word2idx, DEV_DATASET, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 20)\n",
      "(99, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(83, 20)\n",
      "(83, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(93, 20)\n",
      "(93, 20)\n",
      "(91, 20)\n",
      "(91, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(93, 20)\n",
      "(93, 20)\n",
      "(95, 20)\n",
      "(95, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(89, 20)\n",
      "(89, 20)\n",
      "(95, 20)\n",
      "(95, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(92, 20)\n",
      "(92, 20)\n",
      "(88, 20)\n",
      "(88, 20)\n",
      "(92, 20)\n",
      "(92, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(94, 20)\n",
      "(94, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(94, 20)\n",
      "(94, 20)\n",
      "(92, 20)\n",
      "(92, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(94, 20)\n",
      "(94, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(95, 20)\n",
      "(95, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(84, 20)\n",
      "(84, 20)\n",
      "(95, 20)\n",
      "(95, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(99, 20)\n",
      "(99, 20)\n",
      "(93, 20)\n",
      "(93, 20)\n",
      "(85, 20)\n",
      "(85, 20)\n",
      "(95, 20)\n",
      "(95, 20)\n",
      "(98, 20)\n",
      "(98, 20)\n",
      "(97, 20)\n",
      "(97, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n",
      "(96, 20)\n"
     ]
    }
   ],
   "source": [
    "j = 0 \n",
    "for tx, tx_mask, ty, ty_mask in list(train_stream.get_epoch_iterator()):\n",
    "    l = len([idx2word[i] for i in tx[:, 0]])\n",
    "    if l == 19:\n",
    "        print(j)\n",
    "    j += 1\n",
    "    print(tx.shape)\n",
    "    print(ty.shape)\n",
    "    if tx.shape != ty.shape:\n",
    "        print('ERROR')\n",
    "        print(j)\n",
    "        print(tx.shape)\n",
    "        print(ty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "\n",
    "class RNNLM(Initializable):\n",
    "    \"\"\"Model for character level LM\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, state_dim, output_dim, **kwargs):\n",
    "        super(RNNLM, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.lookup = LookupTable(name='embeddings')\n",
    "        self.transition = GatedRecurrent(activation=Tanh(), dim=state_dim)\n",
    "        \n",
    "        self.fork = Fork(\n",
    "            [name for name in self.transition.apply.sequences\n",
    "             if name != 'mask'], prototype=Linear(), name='fork')\n",
    "\n",
    "        # output layer -- this may need to be changed\n",
    "        self.output_layer = Linear(name='output_layer')\n",
    "        \n",
    "        self.children = [self.lookup, self.transition,\n",
    "                         self.fork, self.output_layer]\n",
    "\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.lookup.length = self.vocab_size\n",
    "        self.lookup.dim = self.embedding_dim\n",
    "\n",
    "        self.fork.input_dim = self.embedding_dim\n",
    "        self.fork.output_dims = [self.transition.get_dim(name)\n",
    "                                 for name in self.fork.output_names]\n",
    "        \n",
    "        self.output_layer.input_dim = self.state_dim\n",
    "        self.output_layer.output_dim = self.output_dim\n",
    "\n",
    "    def cost(self, x, x_mask, y, y_mask):\n",
    "        \n",
    "        representation = self.lookup.apply(x)\n",
    "        \n",
    "        states = self.transition.apply(**merge(self.fork.apply(representation, as_dict=True), {'mask': x_mask}))\n",
    "        \n",
    "        # get cost from output layer, transform inputs as necessary\n",
    "        states_shape = states.shape\n",
    "        states_dim1 = states_shape[0] * states_shape[1]\n",
    "        states_dim2 = states_shape[2]\n",
    "\n",
    "        y_hat = Softmax().apply(\n",
    "            self.output_layer.apply(states.reshape((states_dim1, states_dim2))))\n",
    "\n",
    "        y_preds = y_hat.argmax(axis=1)\n",
    "\n",
    "        y = y.flatten()\n",
    "        costs = T.nnet.categorical_crossentropy(y_hat, y)\n",
    "\n",
    "        flat_y_mask = y_mask.flatten()\n",
    "\n",
    "        # here we are zeroing the fake costs for masked parts of the predictions, and dividing by the number of actual\n",
    "        # instances\n",
    "        final_cost = T.sum(costs * flat_y_mask) / T.sum(flat_y_mask)\n",
    "        \n",
    "        return final_cost\n",
    "    \n",
    "    # TODO: implement predict and sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "STATE_DIM=100\n",
    "EMBEDDING_DIM=52\n",
    "OUTPUT_DIM=len(word2idx)\n",
    "\n",
    "test_rnnlm = RNNLM(VOCAB_SIZE, EMBEDDING_DIM, STATE_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize for testing\n",
    "test_rnnlm.weights_init = IsotropicGaussian(0.1)\n",
    "test_rnnlm.biases_init = Constant(0)\n",
    "test_rnnlm.push_initialization_config()\n",
    "test_rnnlm.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make symbolic variables\n",
    "x = T.lmatrix(\"x\")\n",
    "x_mask = T.matrix(\"x_mask\")\n",
    "y = T.lmatrix(\"y\")\n",
    "y_mask = T.matrix(\"y_mask\")\n",
    "\n",
    "cost = test_rnnlm.cost(x, x_mask, y, y_mask)\n",
    "cost.name = 'minibatch_cost'\n",
    "\n",
    "# test_cost_func = theano.function([x, x_mask, y, y_mask], test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.algorithms:Taking the cost gradient\n",
      "INFO:blocks.algorithms:The cost gradient computation graph is built\n",
      "INFO:root:Parameters:\n",
      "[('/output_layer.b', (52,)),\n",
      " ('/gatedrecurrent.initial_state', (100,)),\n",
      " ('/fork/fork_gate_inputs.b', (200,)),\n",
      " ('/fork/fork_gate_inputs.W', (52, 200)),\n",
      " ('/fork/fork_inputs.b', (100,)),\n",
      " ('/fork/fork_inputs.W', (52, 100)),\n",
      " ('/embeddings.W', (52, 52)),\n",
      " ('/gatedrecurrent.state_to_gates', (100, 200)),\n",
      " ('/gatedrecurrent.state_to_state', (100, 100)),\n",
      " ('/output_layer.W', (100, 52))]\n",
      "DEBUG:blocks.monitoring.evaluators:variable to evaluate: minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Using the default  (average over minibatches) aggregation scheme for minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Compiling initialization and readout functions\n",
      "DEBUG:blocks.monitoring.evaluators:Initialization and readout functions compiled\n",
      "DEBUG:blocks.monitoring.evaluators:variable to evaluate: minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Compiling initialization and readout functions\n",
      "DEBUG:blocks.monitoring.evaluators:Initialization and readout functions compiled\n",
      "INFO:blocks.main_loop:Entered the main loop\n",
      "INFO:blocks.algorithms:Initializing the training algorithm\n",
      "INFO:blocks.algorithms:The training algorithm is initialized\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t valid_minibatch_cost: 3.95420837402\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t minibatch_cost: 3.953820467\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t minibatch_cost: 3.93443369865\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t minibatch_cost: 3.91236114502\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t minibatch_cost: 3.88954806328\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t minibatch_cost: 3.86123895645\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t minibatch_cost: 3.8275578022\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 7\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 7:\n",
      "\t minibatch_cost: 3.78624892235\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 8\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 8:\n",
      "\t minibatch_cost: 3.72195243835\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 9\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 9:\n",
      "\t minibatch_cost: 3.61965918541\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t minibatch_cost: 3.43964910507\n",
      "\t valid_minibatch_cost: 3.29560899734\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 11\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 11:\n",
      "\t minibatch_cost: 3.23203635216\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 12\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 12:\n",
      "\t minibatch_cost: 3.13939285278\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 13\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 13:\n",
      "\t minibatch_cost: 3.08510947227\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 14\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 14:\n",
      "\t minibatch_cost: 3.10659980774\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 15\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 15:\n",
      "\t minibatch_cost: 3.11994314194\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 16\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 16:\n",
      "\t minibatch_cost: 3.10387349129\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 17\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 17:\n",
      "\t minibatch_cost: 3.02397632599\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 18\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 18:\n",
      "\t minibatch_cost: 3.03322196007\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 19\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 19:\n",
      "\t minibatch_cost: 3.03987407684\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t minibatch_cost: 3.04270458221\n",
      "\t valid_minibatch_cost: 3.090726614\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 21\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 21:\n",
      "\t minibatch_cost: 3.07448124886\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t minibatch_cost: 2.99219417572\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 23\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 23:\n",
      "\t minibatch_cost: 2.99840140343\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 24\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 24:\n",
      "\t minibatch_cost: 3.03172945976\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 25\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 25:\n",
      "\t minibatch_cost: 3.01475143433\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 26\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 26:\n",
      "\t minibatch_cost: 3.02749252319\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 27\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 27:\n",
      "\t minibatch_cost: 3.01501536369\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 28\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 28:\n",
      "\t minibatch_cost: 3.0021841526\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 29\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 29:\n",
      "\t minibatch_cost: 3.04321408272\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 30\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 30:\n",
      "\t minibatch_cost: 3.00761699677\n",
      "\t valid_minibatch_cost: 3.02150440216\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 31\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 31:\n",
      "\t minibatch_cost: 2.96621108055\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 32\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 32:\n",
      "\t minibatch_cost: 2.99713969231\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 33\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 33:\n",
      "\t minibatch_cost: 2.97551703453\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 34\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 34:\n",
      "\t minibatch_cost: 3.03825712204\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 35\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 35:\n",
      "\t minibatch_cost: 2.93880224228\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 36\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 36:\n",
      "\t minibatch_cost: 2.92759537697\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 37\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 37:\n",
      "\t minibatch_cost: 2.99109125137\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 38\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 38:\n",
      "\t minibatch_cost: 2.94460606575\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 39\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 39:\n",
      "\t minibatch_cost: 2.93052244186\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 40\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 40:\n",
      "\t minibatch_cost: 2.95222306252\n",
      "\t valid_minibatch_cost: 3.0037150383\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 41\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 41:\n",
      "\t minibatch_cost: 2.9520816803\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 42\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 42:\n",
      "\t minibatch_cost: 2.92417216301\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 43\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 43:\n",
      "\t minibatch_cost: 2.99135947227\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 44\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 44:\n",
      "\t minibatch_cost: 2.92774057388\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.graph import ComputationGraph, apply_noise\n",
    "from blocks.model import Model\n",
    "from blocks.algorithms import (GradientDescent, Scale,\n",
    "                               StepClipping, CompositeRule, AdaDelta, Adam)\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring, DataStreamMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.stopping import FinishIfNoImprovementAfter\n",
    "from blocks.graph import apply_dropout, apply_noise\n",
    "\n",
    "# create the Blocks main loop\n",
    "cost_cg = ComputationGraph(cost)\n",
    "model = Model(cost)\n",
    "\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cost_cg.parameters,\n",
    "    step_rule=CompositeRule([StepClipping(1.),\n",
    "                             AdaDelta()])\n",
    ")\n",
    "\n",
    "parameters = model.get_parameter_dict()\n",
    "logger.info(\"Parameters:\\n\" +\n",
    "                pprint.pformat(\n",
    "                    [(key, value.get_value().shape) for key, value in parameters.items()],\n",
    "                    width=120))\n",
    "\n",
    "# Note that observables also get added to the log -- this is useful for post-processing\n",
    "observables = [cost]\n",
    "\n",
    "\n",
    "extensions = [\n",
    "#     Timing(every_n_batches=1),\n",
    "    TrainingDataMonitoring(observables, after_batch=True),\n",
    "    DataStreamMonitoring(\n",
    "        [cost],\n",
    "        data_stream=dev_stream,\n",
    "        prefix=\"valid\",\n",
    "        every_n_batches=10\n",
    "    ),\n",
    "    FinishAfter(after_n_batches=10000),\n",
    "    Printing(every_n_batches=1, after_epoch=False)\n",
    "]\n",
    "\n",
    "\n",
    "main_loop = MainLoop(\n",
    "    model=model,\n",
    "    data_stream=train_stream,\n",
    "    algorithm=algorithm,\n",
    "    extensions=extensions\n",
    ")\n",
    "\n",
    "main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
