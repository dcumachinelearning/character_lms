{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a character level Fuel dataset from text file\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from toolz import merge\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from toolz import merge\n",
    "import pandas as pd\n",
    "from six import add_metaclass\n",
    "from picklable_itertools.extras import equizip\n",
    "from blocks.bricks.recurrent import (GatedRecurrent, Bidirectional)\n",
    "from blocks.initialization import IsotropicGaussian, Constant, Orthogonal\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from blocks.bricks import Initializable\n",
    "from blocks.bricks.base import application, Brick, lazy\n",
    "from blocks.bricks import Tanh, Linear, MLP\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.utils import (shared_floatx_nans, dict_union)\n",
    "from blocks.roles import add_role, WEIGHT\n",
    "from blocks.bricks import Tanh, Linear, Softmax, MLP\n",
    "from blocks.extensions.saveload import Checkpoint\n",
    "\n",
    "\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to your training data\n",
    "TRAINING_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.train.txt'\n",
    "DEV_DATASET = '/home/chris/projects/machine_learning/dcu-character-lms/data/paul_graham_essays.dev.txt'\n",
    "\n",
    "UNKNOWN_TOKEN = '|'\n",
    "EOS_TOKEN = '`'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    all_symbols = set()\n",
    "    with codecs.open(dataset, encoding='utf8') as inp: \n",
    "        for l in inp.read().strip().split('\\n'):\n",
    "            all_symbols.update(l)\n",
    "    return all_symbols\n",
    "    \n",
    "    \n",
    "def get_y_file(dataset):\n",
    "    \n",
    "    # a file to hold the Y representation of the dataset\n",
    "    y_tmp_file = os.path.join(os.path.basename(dataset), dataset+'_temp_y.txt')\n",
    "    \n",
    "    with codecs.open(dataset, encoding='utf8') as inp:\n",
    "        with codecs.open(\n",
    "            y_tmp_file, 'wb',encoding='utf8') as y_out:\n",
    "            for l in inp.read().strip().split('\\n'):\n",
    "                y_seq = l[1:] + EOS_TOKEN\n",
    "                assert len(l) == len(y_seq)\n",
    "    \n",
    "                y_out.write(''.join(y_seq) + '\\n')\n",
    "\n",
    "    return y_tmp_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharTextFile(TextFile):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CharTextFile, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    # override parent method to only do rstrip instead of strip\n",
    "    def get_data(self, state=None, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        sentence = next(state)\n",
    "        if self.preprocess is not None:\n",
    "            sentence = self.preprocess(sentence)\n",
    "        data = [self.dictionary[self.bos_token]] if self.bos_token else []\n",
    "        if self.level == 'word':\n",
    "            data.extend(self.dictionary.get(word,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for word in sentence.split())\n",
    "        else:\n",
    "            data.extend(self.dictionary.get(char,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for char in sentence.rstrip())\n",
    "        if self.eos_token:\n",
    "            data.append(self.dictionary[self.eos_token])\n",
    "        return (data,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add an unknown token in case we observe a new character at prediction time\n",
    "vocab = get_vocab(TRAINING_DATASET)\n",
    "vocab.update([UNKNOWN_TOKEN, EOS_TOKEN])\n",
    "word2idx = {v:k for k,v in enumerate(vocab)}\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "\n",
    "# axis swapping so that we get (time, batch, features)\n",
    "def swapaxes(data):\n",
    "    \"\"\"Switch the axes in the sequence parts of the data tuple\"\"\"\n",
    "    return tuple(array.swapaxes(0,1) for array in data)\n",
    "\n",
    "class _too_long(object):\n",
    "    \"\"\"Filters sequences longer than given sequence length.\"\"\"\n",
    "    def __init__(self, seq_len=50):\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __call__(self, sentence_pair):\n",
    "        return all([len(sentence) <= self.seq_len\n",
    "                    for sentence in sentence_pair])\n",
    "\n",
    "\n",
    "def get_stream(vocab, x_file,\n",
    "               seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "    \"\"\"Prepares the data stream.\"\"\"\n",
    "    \n",
    "    y_file = get_y_file(x_file)\n",
    "\n",
    "    def _length(item):\n",
    "        \"\"\"Item is assumed to be (x,y)\"\"\"\n",
    "        return len(item[0])\n",
    "    \n",
    "    # Get text files from both source and target\n",
    "    X = CharTextFile([x_file], vocab, bos_token=None, eos_token=None,\n",
    "                 unk_token=unk_token, level='character')\n",
    "\n",
    "    Y = CharTextFile([y_file], vocab, bos_token=None, eos_token=None,\n",
    "                 unk_token=unk_token, level='character')\n",
    "\n",
    "    # Merge them to get x1, x2 pairs\n",
    "    stream = Merge([X.get_example_stream(),\n",
    "                    Y.get_example_stream()],\n",
    "                    ('x', 'y'))\n",
    "\n",
    "    # Filter sequences that are too long\n",
    "    stream = Filter(stream, predicate=_too_long(seq_len=seq_len))\n",
    "\n",
    "    # LOOKAHEAD SORT\n",
    "    # Build a batched version of stream to read k batches ahead\n",
    "#     stream = Batch(stream,\n",
    "#                    iteration_scheme=ConstantScheme(\n",
    "#                    batch_size*sort_k_batches))\n",
    "    \n",
    "    # Sort all samples in the read-ahead batch\n",
    "#     stream = Mapping(stream, SortMapping(_length))\n",
    "\n",
    "    # Convert it into a stream again\n",
    "#     stream = Unpack(stream)\n",
    "    # END LOOKAHEAD SORT\n",
    "\n",
    "    # Construct batches from the stream with specified batch size\n",
    "    stream = Batch(\n",
    "        stream, iteration_scheme=ConstantScheme(batch_size))\n",
    "\n",
    "    # Pad sequences that are short\n",
    "    masked_stream = Padding(stream)\n",
    "    \n",
    "    # transpose tensors to get (time, batch, features)\n",
    "    masked_stream = Mapping(masked_stream, swapaxes)\n",
    "\n",
    "    return masked_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def get_stream(vocab, x_file, y_file,\n",
    "#                seq_len=50, batch_size=20, sort_k_batches=5, unk_token=\"`\", **kwargs):\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "MAX_SEQ_LEN = 200\n",
    "\n",
    "train_stream = get_stream(word2idx, TRAINING_DATASET, batch_size=BATCH_SIZE, seq_len=MAX_SEQ_LEN)\n",
    "dev_stream = get_stream(word2idx, DEV_DATASET, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95, 10)\n",
      "(95, 10)\n",
      "(99, 10)\n",
      "(99, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(88, 10)\n",
      "(88, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(79, 10)\n",
      "(79, 10)\n",
      "(83, 10)\n",
      "(83, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(81, 10)\n",
      "(81, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(88, 10)\n",
      "(88, 10)\n",
      "(81, 10)\n",
      "(81, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(90, 10)\n",
      "(90, 10)\n",
      "(91, 10)\n",
      "(91, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(89, 10)\n",
      "(89, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(75, 10)\n",
      "(75, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(87, 10)\n",
      "(87, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(87, 10)\n",
      "(87, 10)\n",
      "(91, 10)\n",
      "(91, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(86, 10)\n",
      "(86, 10)\n",
      "(89, 10)\n",
      "(89, 10)\n",
      "(89, 10)\n",
      "(89, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(75, 10)\n",
      "(75, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(85, 10)\n",
      "(85, 10)\n",
      "(88, 10)\n",
      "(88, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(90, 10)\n",
      "(90, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(90, 10)\n",
      "(90, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(86, 10)\n",
      "(86, 10)\n",
      "(76, 10)\n",
      "(76, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(89, 10)\n",
      "(89, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(87, 10)\n",
      "(87, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(91, 10)\n",
      "(91, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(77, 10)\n",
      "(77, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(91, 10)\n",
      "(91, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(80, 10)\n",
      "(80, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(85, 10)\n",
      "(85, 10)\n",
      "(82, 10)\n",
      "(82, 10)\n",
      "(84, 10)\n",
      "(84, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(72, 10)\n",
      "(72, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(86, 10)\n",
      "(86, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(99, 10)\n",
      "(99, 10)\n",
      "(82, 10)\n",
      "(82, 10)\n",
      "(93, 10)\n",
      "(93, 10)\n",
      "(85, 10)\n",
      "(85, 10)\n",
      "(81, 10)\n",
      "(81, 10)\n",
      "(94, 10)\n",
      "(94, 10)\n",
      "(95, 10)\n",
      "(95, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(98, 10)\n",
      "(98, 10)\n",
      "(97, 10)\n",
      "(97, 10)\n",
      "(84, 10)\n",
      "(84, 10)\n",
      "(75, 10)\n",
      "(75, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(96, 10)\n",
      "(92, 10)\n",
      "(92, 10)\n"
     ]
    }
   ],
   "source": [
    "j = 0 \n",
    "for tx, tx_mask, ty, ty_mask in list(train_stream.get_epoch_iterator()):\n",
    "    l = len([idx2word[i] for i in tx[:, 0]])\n",
    "    if l == 19:\n",
    "        print(j)\n",
    "    j += 1\n",
    "    print(tx.shape)\n",
    "    print(ty.shape)\n",
    "    if tx.shape != ty.shape:\n",
    "        print('ERROR')\n",
    "        print(j)\n",
    "        print(tx.shape)\n",
    "        print(ty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "\n",
    "class RNNLM(Initializable):\n",
    "    \"\"\"Model for character level LM\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, state_dim, output_dim, **kwargs):\n",
    "        super(RNNLM, self).__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "#         self._sequence_names = list(transition.apply.sequences)\n",
    "#         self._state_names = list(transition.apply.states)\n",
    "#         self._context_names = list(transition.apply.contexts)\n",
    "        \n",
    "        self.lookup = LookupTable(name='embeddings')\n",
    "        self.transition = GatedRecurrent(activation=Tanh(), dim=state_dim)\n",
    "        \n",
    "        self.fork = Fork(\n",
    "            [name for name in self.transition.apply.sequences\n",
    "             if name != 'mask'], prototype=Linear(), name='fork')\n",
    "\n",
    "        # output layer -- this may need to be changed\n",
    "        self.output_layer = Linear(name='output_layer')\n",
    "        \n",
    "        self.children = [self.lookup, self.transition,\n",
    "                         self.fork, self.output_layer]\n",
    "\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.lookup.length = self.vocab_size\n",
    "        self.lookup.dim = self.embedding_dim\n",
    "\n",
    "        self.fork.input_dim = self.embedding_dim\n",
    "        self.fork.output_dims = [self.transition.get_dim(name)\n",
    "                                 for name in self.fork.output_names]\n",
    "        \n",
    "        self.output_layer.input_dim = self.state_dim\n",
    "        self.output_layer.output_dim = self.output_dim\n",
    "\n",
    "    def cost(self, x, x_mask, y, y_mask):\n",
    "        \n",
    "        representation = self.lookup.apply(x)\n",
    "        \n",
    "        states = self.transition.apply(**merge(self.fork.apply(representation, as_dict=True), {'mask': x_mask}))\n",
    "        \n",
    "        # get cost from output layer, transform inputs as necessary\n",
    "        states_shape = states.shape\n",
    "        states_dim1 = states_shape[0] * states_shape[1]\n",
    "        states_dim2 = states_shape[2]\n",
    "\n",
    "        y_hat = Softmax().apply(\n",
    "            self.output_layer.apply(states.reshape((states_dim1, states_dim2))))\n",
    "\n",
    "        y_preds = y_hat.argmax(axis=1)\n",
    "\n",
    "        y = y.flatten()\n",
    "        costs = T.nnet.categorical_crossentropy(y_hat, y)\n",
    "\n",
    "        flat_y_mask = y_mask.flatten()\n",
    "\n",
    "        # here we are zeroing the fake costs for masked parts of the predictions, and dividing by the number of actual\n",
    "        # instances\n",
    "        final_cost = T.sum(costs * flat_y_mask) / T.sum(flat_y_mask)\n",
    "        \n",
    "        return final_cost\n",
    "    \n",
    "    # TODO: really we should just compute one step of the recurrent transition each time,\n",
    "    # but that means we are going to scan over sample instead, so we are adding another \"layer\" \n",
    "    # to the recurrent transition\n",
    "    # we want to implement a repeated sampling process where the output is fed back in\n",
    "    # this is also probably @recurrent\n",
    "    # remember: n_steps, iterate=False\n",
    "    \n",
    "    # We need to add the output to the states of this transition -- can be done using the @sample.delegate\n",
    "    # decorator\n",
    "    \n",
    "# FROM NMT decoder model\n",
    "#         @application\n",
    "#     def generate(self, source_sentence, representation, **kwargs):\n",
    "#         return self.sequence_generator.generate(\n",
    "#             n_steps=2 * source_sentence.shape[1],\n",
    "#             batch_size=source_sentence.shape[0],\n",
    "#             attended=representation,\n",
    "#             attended_mask=tensor.ones(source_sentence.shape).T,\n",
    "#             **kwargs)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     @recurrent\n",
    "#     def sample(previous_output, sample_length=10):\n",
    "#         \"\"\"Sample from the model\n",
    "        \n",
    "#         x: int -- the seed character\n",
    "        \n",
    "#         \"\"\"\n",
    "        \n",
    "#         # wrap start_x to make dims correct\n",
    "# #         x = np.array([start_x]).reshape((1, 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "#         for t in range(sample_length):\n",
    "#             representation = self.lookup.apply(current_seq)\n",
    "\n",
    "#             states = self.transition.apply(**merge(self.fork.apply(representation, as_dict=True)))\n",
    "\n",
    "#             last_state = states[-1]\n",
    "            \n",
    "#             # get cost from output layer, transform inputs as necessary\n",
    "# #             states_shape = states.shape\n",
    "# #             states_dim1 = states_shape[0] * states_shape[1]\n",
    "# #             states_dim2 = states_shape[2]\n",
    "\n",
    "#             y_hat = Softmax().apply(\n",
    "#                 self.output_layer.apply(last_state))\n",
    "\n",
    "#             # TODO: get a random sample parameterized by softmax distribution\n",
    "#             y_pred = y_hat.argmax(axis=1)\n",
    "# #             current_seq.concatenate(y_pred, axis=)\n",
    "            \n",
    "#         return current_seq\n",
    "    \n",
    "    \n",
    "#  current_states = self.transition.apply(\n",
    "#             iterate=False, as_list=True,\n",
    "#             **dict_union(sequences, kwargs))\n",
    "    \n",
    "    # Chris: Note the `recurrent` annotation here\n",
    "#     @recurrent\n",
    "#     def generate(self, outputs, **kwargs):\n",
    "#         \"\"\"A sequence generation step.\n",
    "\n",
    "#         Chris: how can we constrain this step to use the target word input at the current timestep?\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         outputs : :class:`~tensor.TensorVariable`\n",
    "#             The outputs from the previous step.\n",
    "\n",
    "#         Notes\n",
    "#         -----\n",
    "#         The contexts, previous states and glimpses are expected as keyword\n",
    "#         arguments.\n",
    "\n",
    "#         \"\"\"\n",
    "#         states = dict_subset(kwargs, self._state_names)\n",
    "#         # masks in context are optional (e.g. `attended_mask`)\n",
    "#         # contexts = dict_subset(kwargs, self._context_names, must_have=False)\n",
    "        \n",
    "\n",
    "#         next_glimpses = self.transition.take_glimpses(\n",
    "#             as_dict=True, **dict_union(states, glimpses, contexts))\n",
    "#         next_readouts = self.readout.readout(\n",
    "#             feedback=self.readout.feedback(outputs),\n",
    "#             **dict_union(states, next_glimpses, contexts))\n",
    "#         next_outputs = self.readout.emit(next_readouts)\n",
    "#         next_costs = self.readout.cost(next_readouts, next_outputs)\n",
    "#         next_feedback = self.readout.feedback(next_outputs)\n",
    "#         next_inputs = (self.fork.apply(next_feedback, as_dict=True)\n",
    "#                        if self.fork else {'feedback': next_feedback})\n",
    "#         next_states = self.transition.compute_states(\n",
    "#             as_list=True,\n",
    "#             **dict_union(next_inputs, states, next_glimpses, contexts))\n",
    "#         return (next_states + [next_outputs] +\n",
    "#                 list(next_glimpses.values()) + [next_costs])\n",
    "\n",
    "#     @generate.delegate\n",
    "#     def generate_delegate(self):\n",
    "#         return self.transition.apply\n",
    "\n",
    "#     @generate.property('states')\n",
    "#     def generate_states(self):\n",
    "#         return self._state_names + ['outputs']\n",
    "\n",
    "#     @generate.property('outputs')\n",
    "#     def generate_outputs(self):\n",
    "#         return (self._state_names + ['outputs'] +\n",
    "#                 self._glimpse_names + ['costs'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2idx)\n",
    "STATE_DIM=100\n",
    "EMBEDDING_DIM=52\n",
    "OUTPUT_DIM=len(word2idx)\n",
    "SAVE_PATH='test_model_checkpoint'\n",
    "\n",
    "test_rnnlm = RNNLM(VOCAB_SIZE, EMBEDDING_DIM, STATE_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize for testing\n",
    "test_rnnlm.weights_init = IsotropicGaussian(0.1)\n",
    "test_rnnlm.biases_init = Constant(0)\n",
    "test_rnnlm.push_initialization_config()\n",
    "test_rnnlm.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make symbolic variables\n",
    "x = T.lmatrix(\"x\")\n",
    "x_mask = T.matrix(\"x_mask\")\n",
    "y = T.lmatrix(\"y\")\n",
    "y_mask = T.matrix(\"y_mask\")\n",
    "\n",
    "cost = test_rnnlm.cost(x, x_mask, y, y_mask)\n",
    "cost.name = 'minibatch_cost'\n",
    "\n",
    "# test_cost_func = theano.function([x, x_mask, y, y_mask], test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.algorithms:Taking the cost gradient\n",
      "INFO:blocks.algorithms:The cost gradient computation graph is built\n",
      "INFO:root:Parameters:\n",
      "[('/output_layer.b', (52,)),\n",
      " ('/gatedrecurrent.initial_state', (100,)),\n",
      " ('/fork/fork_gate_inputs.b', (200,)),\n",
      " ('/fork/fork_gate_inputs.W', (52, 200)),\n",
      " ('/fork/fork_inputs.b', (100,)),\n",
      " ('/fork/fork_inputs.W', (52, 100)),\n",
      " ('/embeddings.W', (52, 52)),\n",
      " ('/gatedrecurrent.state_to_gates', (100, 200)),\n",
      " ('/gatedrecurrent.state_to_state', (100, 100)),\n",
      " ('/output_layer.W', (100, 52))]\n",
      "DEBUG:blocks.monitoring.evaluators:variable to evaluate: minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Using the default  (average over minibatches) aggregation scheme for minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Compiling initialization and readout functions\n",
      "DEBUG:blocks.monitoring.evaluators:Initialization and readout functions compiled\n",
      "DEBUG:blocks.monitoring.evaluators:variable to evaluate: minibatch_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Compiling initialization and readout functions\n",
      "DEBUG:blocks.monitoring.evaluators:Initialization and readout functions compiled\n",
      "INFO:blocks.main_loop:Entered the main loop\n",
      "INFO:blocks.algorithms:Initializing the training algorithm\n",
      "INFO:blocks.algorithms:The training algorithm is initialized\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t valid_minibatch_cost: 3.95278811455\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t minibatch_cost: 3.95370554924\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t minibatch_cost: 3.9307179451\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t minibatch_cost: 3.90563058853\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t minibatch_cost: 3.88091659546\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t minibatch_cost: 3.85534501076\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t minibatch_cost: 3.80460762978\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 7\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 7:\n",
      "\t minibatch_cost: 3.74867725372\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 8\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 8:\n",
      "\t minibatch_cost: 3.65155982971\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 9\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 9:\n",
      "\t minibatch_cost: 3.48032522202\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 10\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 10:\n",
      "\t minibatch_cost: 3.27284049988\n",
      "\t valid_minibatch_cost: 3.24759292603\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 11\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 11:\n",
      "\t minibatch_cost: 3.15241479874\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 12\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 12:\n",
      "\t minibatch_cost: 3.16288423538\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 13\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 13:\n",
      "\t minibatch_cost: 3.13193655014\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 14\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 14:\n",
      "\t minibatch_cost: 3.11018872261\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 15\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 15:\n",
      "\t minibatch_cost: 3.01767611504\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 16\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 16:\n",
      "\t minibatch_cost: 3.08597040176\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 17\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 17:\n",
      "\t minibatch_cost: 3.04741334915\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 18\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 18:\n",
      "\t minibatch_cost: 3.03473639488\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 19\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 19:\n",
      "\t minibatch_cost: 3.02148342133\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 20\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 20:\n",
      "\t minibatch_cost: 2.98765850067\n",
      "\t valid_minibatch_cost: 3.08249235153\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 21\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 21:\n",
      "\t minibatch_cost: 3.02343082428\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 22\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 22:\n",
      "\t minibatch_cost: 3.00692176819\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 23\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 23:\n",
      "\t minibatch_cost: 2.98998785019\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 24\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 24:\n",
      "\t minibatch_cost: 3.06443333626\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 25\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 25:\n",
      "\t minibatch_cost: 2.9682559967\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 26\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 26:\n",
      "\t minibatch_cost: 2.97153091431\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 27\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 27:\n",
      "\t minibatch_cost: 3.05431079865\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 28\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 28:\n",
      "\t minibatch_cost: 2.99536705017\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 29\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 29:\n",
      "\t minibatch_cost: 3.02762246132\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 30\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 30:\n",
      "\t minibatch_cost: 2.99631357193\n",
      "\t valid_minibatch_cost: 3.03388547897\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 31\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 31:\n",
      "\t minibatch_cost: 3.02467536926\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 32\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 32:\n",
      "\t minibatch_cost: 3.06289625168\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 33\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 33:\n",
      "\t minibatch_cost: 2.89393019676\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 34\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 34:\n",
      "\t minibatch_cost: 2.97342252731\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 35\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 35:\n",
      "\t minibatch_cost: 2.95307970047\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 36\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 36:\n",
      "\t minibatch_cost: 2.99285387993\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 37\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 37:\n",
      "\t minibatch_cost: 2.95280814171\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 38\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 38:\n",
      "\t minibatch_cost: 2.9381210804\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 39\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 39:\n",
      "\t minibatch_cost: 2.96629405022\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 40\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 40:\n",
      "\t minibatch_cost: 2.94725823402\n",
      "\t valid_minibatch_cost: 2.98470449448\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 41\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 41:\n",
      "\t minibatch_cost: 2.99842619896\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 42\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 42:\n",
      "\t minibatch_cost: 2.95237517357\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 43\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 43:\n",
      "\t minibatch_cost: 2.93175005913\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 44\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 44:\n",
      "\t minibatch_cost: 2.89471292496\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 45\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 45:\n",
      "\t minibatch_cost: 2.8894238472\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 46\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 46:\n",
      "\t minibatch_cost: 2.90625977516\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 47\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 47:\n",
      "\t minibatch_cost: 2.95728588104\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 48\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 48:\n",
      "\t minibatch_cost: 2.90445780754\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 49\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 49:\n",
      "\t minibatch_cost: 2.87337255478\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 50\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 50:\n",
      "\t minibatch_cost: 2.86586356163\n",
      "\t valid_minibatch_cost: 2.9433016777\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 51\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 51:\n",
      "\t minibatch_cost: 2.86270713806\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 52\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 52:\n",
      "\t minibatch_cost: 2.9904551506\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 53\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 53:\n",
      "\t minibatch_cost: 2.90830516815\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 54\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 54:\n",
      "\t minibatch_cost: 2.89663481712\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 55\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 55:\n",
      "\t minibatch_cost: 2.93983483315\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 56\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 56:\n",
      "\t minibatch_cost: 2.92996001244\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 57\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 57:\n",
      "\t minibatch_cost: 2.98799848557\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 58\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 58:\n",
      "\t minibatch_cost: 2.8739593029\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 59\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 59:\n",
      "\t minibatch_cost: 2.82209253311\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 60\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 60:\n",
      "\t minibatch_cost: 2.91804623604\n",
      "\t valid_minibatch_cost: 2.8881881237\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 61\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 61:\n",
      "\t minibatch_cost: 2.86258888245\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 62\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 62:\n",
      "\t minibatch_cost: 2.79521274567\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 63\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 63:\n",
      "\t minibatch_cost: 2.87696433067\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 64\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 64:\n",
      "\t minibatch_cost: 2.8617618084\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 65\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 65:\n",
      "\t minibatch_cost: 2.8235104084\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 66\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 66:\n",
      "\t minibatch_cost: 2.79404139519\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 67\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 67:\n",
      "\t minibatch_cost: 2.92981529236\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 68\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 68:\n",
      "\t minibatch_cost: 2.8678817749\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 69\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 69:\n",
      "\t minibatch_cost: 2.75048947334\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 70\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 70:\n",
      "\t minibatch_cost: 2.71329331398\n",
      "\t valid_minibatch_cost: 2.80474162102\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 71\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 71:\n",
      "\t minibatch_cost: 2.69712424278\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 72\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 72:\n",
      "\t minibatch_cost: 2.72845435143\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 73\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 73:\n",
      "\t minibatch_cost: 2.77368831635\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 74\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 74:\n",
      "\t minibatch_cost: 2.80935740471\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 75\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 75:\n",
      "\t minibatch_cost: 2.72444796562\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 76\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 76:\n",
      "\t minibatch_cost: 2.69024825096\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 77\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 77:\n",
      "\t minibatch_cost: 2.6930437088\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 78\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 78:\n",
      "\t minibatch_cost: 2.70361971855\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 79\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 79:\n",
      "\t minibatch_cost: 2.72320485115\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 80\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 80:\n",
      "\t minibatch_cost: 2.69310736656\n",
      "\t valid_minibatch_cost: 2.75777173042\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 81\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 81:\n",
      "\t minibatch_cost: 2.63103318214\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 82\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 82:\n",
      "\t minibatch_cost: 2.75622749329\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 83\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 83:\n",
      "\t minibatch_cost: 2.58689403534\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 84\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 84:\n",
      "\t minibatch_cost: 2.61650943756\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 85\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 85:\n",
      "\t minibatch_cost: 2.70105481148\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 86\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 86:\n",
      "\t minibatch_cost: 2.74600481987\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 87\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 87:\n",
      "\t minibatch_cost: 2.62307572365\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 88\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 88:\n",
      "\t minibatch_cost: 2.54978466034\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 89\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 89:\n",
      "\t minibatch_cost: 2.62500596046\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 90\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 90:\n",
      "\t minibatch_cost: 2.60757708549\n",
      "\t valid_minibatch_cost: 2.68048667908\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 91\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 91:\n",
      "\t minibatch_cost: 2.62952423096\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 92\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 92:\n",
      "\t minibatch_cost: 2.61164927483\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 93\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 93:\n",
      "\t minibatch_cost: 2.55953335762\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 94\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 94:\n",
      "\t minibatch_cost: 2.60721921921\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 95\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 95:\n",
      "\t minibatch_cost: 2.57955050468\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 96\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 96:\n",
      "\t minibatch_cost: 2.70359063148\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 97\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 97:\n",
      "\t minibatch_cost: 2.52852058411\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 98\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 98:\n",
      "\t minibatch_cost: 2.64542388916\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data started\n",
      "INFO:blocks.extensions.monitoring:Monitoring on auxiliary data finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 99\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 99:\n",
      "\t minibatch_cost: 2.67812466621\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/blocks/serialization.py:85: UserWarning: WARNING: Main loop depends on the function `RNNLM` in `__main__` namespace.\n",
      "\n",
      "Because of limitations to pickling, this means that you will not be able to resume your model outside of a namespace containing this function. In other words, you can only call `continue_training` from within this script.\n",
      "  MAIN_MODULE_WARNING.format(kwargs.get('name', obj.__name__))\n",
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/blocks/serialization.py:85: UserWarning: WARNING: Main loop depends on the function `swapaxes` in `__main__` namespace.\n",
      "\n",
      "Because of limitations to pickling, this means that you will not be able to resume your model outside of a namespace containing this function. In other words, you can only call `continue_training` from within this script.\n",
      "  MAIN_MODULE_WARNING.format(kwargs.get('name', obj.__name__))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 100\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 100:\n",
      "\t minibatch_cost: 2.6717004776\n",
      "\t training_finish_requested: True\n",
      "\t valid_minibatch_cost: 2.63111042976\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 100\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 100:\n",
      "\t minibatch_cost: 2.6717004776\n",
      "\t saved_to: ('test_model_checkpoint',)\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\t valid_minibatch_cost: 2.63111042976\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/blocks/serialization.py:85: UserWarning: WARNING: Main loop depends on the function `_too_long` in `__main__` namespace.\n",
      "\n",
      "Because of limitations to pickling, this means that you will not be able to resume your model outside of a namespace containing this function. In other words, you can only call `continue_training` from within this script.\n",
      "  MAIN_MODULE_WARNING.format(kwargs.get('name', obj.__name__))\n",
      "/home/chris/programs/anaconda/lib/python2.7/site-packages/blocks/serialization.py:85: UserWarning: WARNING: Main loop depends on the function `CharTextFile` in `__main__` namespace.\n",
      "\n",
      "Because of limitations to pickling, this means that you will not be able to resume your model outside of a namespace containing this function. In other words, you can only call `continue_training` from within this script.\n",
      "  MAIN_MODULE_WARNING.format(kwargs.get('name', obj.__name__))\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "from blocks.monitoring import aggregation\n",
    "from blocks.graph import ComputationGraph, apply_noise\n",
    "from blocks.model import Model\n",
    "from blocks.algorithms import (GradientDescent, Scale,\n",
    "                               StepClipping, CompositeRule, AdaDelta, Adam)\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring, DataStreamMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.stopping import FinishIfNoImprovementAfter\n",
    "from blocks.graph import apply_dropout, apply_noise\n",
    "\n",
    "# create the Blocks main loop\n",
    "cost_cg = ComputationGraph(cost)\n",
    "model = Model(cost)\n",
    "\n",
    "algorithm = GradientDescent(\n",
    "    cost=cost, parameters=cost_cg.parameters,\n",
    "    step_rule=CompositeRule([StepClipping(1.),\n",
    "                             AdaDelta()])\n",
    ")\n",
    "\n",
    "parameters = model.get_parameter_dict()\n",
    "logger.info(\"Parameters:\\n\" +\n",
    "                pprint.pformat(\n",
    "                    [(key, value.get_value().shape) for key, value in parameters.items()],\n",
    "                    width=120))\n",
    "\n",
    "# Note that observables also get added to the log -- this is useful for post-processing\n",
    "observables = [cost]\n",
    "\n",
    "\n",
    "extensions = [\n",
    "#     Timing(every_n_batches=1),\n",
    "    TrainingDataMonitoring(observables, after_batch=True),\n",
    "    DataStreamMonitoring(\n",
    "        [cost],\n",
    "        data_stream=dev_stream,\n",
    "        prefix=\"valid\",\n",
    "        every_n_batches=10\n",
    "    ),\n",
    "    FinishAfter(after_n_batches=100),\n",
    "    Printing(every_n_batches=1, after_epoch=False),\n",
    "    Checkpoint(SAVE_PATH, every_n_batches=100, save_separately=['log'])\n",
    "]\n",
    "\n",
    "\n",
    "main_loop = MainLoop(\n",
    "    model=model,\n",
    "    data_stream=train_stream,\n",
    "    algorithm=algorithm,\n",
    "    extensions=extensions\n",
    ")\n",
    "\n",
    "main_loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize cost on train and dev over time\n",
    "import cPickle\n",
    "\n",
    "# inspect and visualize the log of a blocks experiment\n",
    "\n",
    "LOG_LOCATION= SAVE_PATH + '_log'\n",
    "\n",
    "text_offset = 200\n",
    "\n",
    "def visualize_log(log_location, title=\"test\"):\n",
    "    log = cPickle.load(open(log_location))\n",
    "    all_keys = set()\n",
    "    for t in log.keys():\n",
    "        all_keys.update(log[t].keys())\n",
    "    \n",
    "    print(all_keys)\n",
    "    interesting_keys = ['minibatch_cost', 'valid_minibatch_cost']\n",
    "\n",
    "    all_points = []\n",
    "    mins = []\n",
    "    maxs = []\n",
    "    for idx, key in enumerate(interesting_keys): \n",
    "        points = [(i, log[i][key]) for i in log.keys() if key in log[i]]\n",
    "        min_val, max_val = (min(points, key=lambda x: x[1]), max(points, key=lambda x: x[1]))\n",
    "        mins.append(min_val)\n",
    "        maxs.append(max_val)\n",
    "        print('key: {}, min = {}, max = {}'.format(key, min_val, max_val))\n",
    "        all_points.append(points)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i,key_points in enumerate(all_points):\n",
    "        name = interesting_keys[i]\n",
    "        x,y = zip(*key_points)\n",
    "        plt.plot(x, y, label=name)\n",
    "        ax.scatter(mins[i][0], mins[i][1], s=70, c='red')\n",
    "        ax.annotate('{}:{}'.format(mins[i][0], round(mins[i][1],3)), xy=(mins[i][0]+text_offset, mins[i][1]))\n",
    "        ax.annotate('{}:{}'.format(maxs[i][0], round(maxs[i][1],3)), xy=(maxs[i][0]+text_offset, maxs[i][1]))\n",
    "        ax.scatter(maxs[i][0], maxs[i][1], s=70, c='green')\n",
    "    plt.legend(loc=4)     \n",
    "    plt.title(title, fontsize=28)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['training_finish_requested', 'saved_to', 'valid_minibatch_cost', 'training_finished', 'minibatch_cost'])\n",
      "key: minibatch_cost, min = (97, array(2.5285205841064453, dtype=float32)), max = (1, array(3.9537055492401123, dtype=float32))\n",
      "key: valid_minibatch_cost, min = (100, array(2.631110429763794, dtype=float32)), max = (0, array(3.9527881145477295, dtype=float32))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAIvCAYAAABHg86bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIl7w/wH46ur9YJGLmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd697b89d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_log(LOG_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IDEA: compile two theano funcs that give us what we need, then iterate through the recurrent transitions manually\n",
    "# func one: compute a prediction and an output for one step of the recurrent transition\n",
    "# if this is the first step, we need to get the initial state, otherwise use the previous state\n",
    "\n",
    "# add_role(self.parameters[2], INITIAL_STATE)\n",
    "\n",
    "\n",
    "\n",
    "# one step from the beginning\n",
    "def one_step(x_i, h0):\n",
    "\n",
    "#     x = tensor.lmatrix('x')\n",
    "#     gi = tensor.matrix('gi')\n",
    "    \n",
    "    \n",
    "    representation = test_rnnlm.lookup.apply(x)\n",
    "    inputs, gate_inputs = test_rnnlm.fork.apply(test_rnnlm.lookup.apply(x))\n",
    "    inputs = inputs[0]\n",
    "    gate_inputs = gate_inputs[0]\n",
    "    \n",
    "    states = test_rnnlm.transition.apply(inputs, gate_inputs, h0, iterate=False)\n",
    "                                         \n",
    "    # get cost from output layer\n",
    "    output = test_rnnlm.output_layer.apply(states)\n",
    "    \n",
    "    y_hat = Softmax().apply(output)\n",
    "    y_preds = y_hat.argmax(axis=1)\n",
    "    \n",
    "    return y_preds\n",
    "    \n",
    "#     h1 = test_rnnlm.apply(x, gi, h0, iterate=False)\n",
    "#     next_h = theano.function(inputs=[h0, x, gi], outputs=[h1])\n",
    "\n",
    "#     h0_val = 0.1 * numpy.array([[1, 1, 0], [0, 1, 1]],\n",
    "#                                dtype=theano.config.floatX)\n",
    "#     x_val = 0.1 * numpy.array([[1, 2, 3], [4, 5, 6]],\n",
    "#                               dtype=theano.config.floatX)\n",
    "#     zi_val = (h0_val + x_val) / 2\n",
    "#     ri_val = -x_val\n",
    "#     W_val = 2 * numpy.ones((3, 3), dtype=theano.config.floatX)\n",
    "\n",
    "#     z_val = numpy.tanh(h0_val.dot(W_val) + zi_val)\n",
    "#     r_val = numpy.tanh(h0_val.dot(W_val) + ri_val)\n",
    "#     h1_val = (z_val * numpy.tanh((r_val * h0_val).dot(W_val) + x_val) +\n",
    "#               (1 - z_val) * h0_val)\n",
    "#     assert_allclose(\n",
    "#         h1_val, next_h(h0_val, x_val, numpy.hstack([zi_val, ri_val]))[0],\n",
    "#         rtol=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h0 = T.matrix('h0')\n",
    "\n",
    "one_step_out = one_step(x, h0)\n",
    "\n",
    "test_f = theano.function([x, h0], one_step_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_h0 = test_rnnlm.transition.parameters[2].get_value()[None, :]\n",
    "test_x = np.array([[45]])\n",
    "\n",
    "best_next_char = int(test_f(test_x, test_h0))\n",
    "idx2word[best_next_char]\n",
    "best_next_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rnnlm.transition.parameters[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ff = theano.function([x], test_rnnlm.lookup.apply(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs, gate_inputs = test_rnnlm.fork.apply(test_rnnlm.lookup.apply(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = inputs[0]\n",
    "gate_inputs = gate_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = test_rnnlm.transition.apply(inputs, gate_inputs, h0, iterate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_func = theano.function([x, h0], states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15337265,  0.35798132, -0.03531885,  0.18557921, -0.08655842,\n",
       "         0.20255089,  0.28872693, -0.29286978,  0.33033705,  0.0659342 ,\n",
       "         0.46630245,  0.21770398, -0.37381312, -0.38199341, -0.21020369,\n",
       "        -0.19557318, -0.17720565,  0.18718858,  0.07564428, -0.33102953,\n",
       "         0.19538087,  0.03622055,  0.28203675, -0.06454989, -0.14904541,\n",
       "        -0.17863753,  0.25632882,  0.2258707 , -0.31578323,  0.26739722,\n",
       "        -0.33664936,  0.01170724,  0.3577036 , -0.07073875,  0.23685446,\n",
       "         0.32173142,  0.08102061, -0.26142463,  0.39205122,  0.07891752,\n",
       "         0.20339084, -0.3481304 ,  0.17199868, -0.24260682,  0.30408949,\n",
       "         0.13543908,  0.27734071,  0.24665681,  0.15021122,  0.12408364,\n",
       "         0.17467679,  0.12253734,  0.08632198,  0.04989602,  0.21178529,\n",
       "        -0.23456544,  0.1086115 , -0.02538755,  0.08242612, -0.25234541,\n",
       "        -0.29412743,  0.02206928, -0.07791054,  0.370588  ,  0.12026836,\n",
       "         0.03632174,  0.11247357, -0.03117642,  0.33042902,  0.35121006,\n",
       "        -0.12760206, -0.25895083, -0.12994853,  0.27075657,  0.1812052 ,\n",
       "        -0.08301655,  0.2604613 , -0.32876617, -0.36995614, -0.20130318,\n",
       "        -0.07792779,  0.16947703, -0.35756874,  0.04232499,  0.28342354,\n",
       "        -0.04173093,  0.1328446 , -0.2458621 , -0.22391576, -0.30589911,\n",
       "         0.31001818,  0.19807196, -0.04453238, -0.09589963, -0.08920233,\n",
       "        -0.04641928,  0.0257898 , -0.2172607 , -0.17255116,  0.19391179]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_h0 = test_rnnlm.transition.parameters[2].get_value()[None, :]\n",
    "test_x = np.array([[26]])\n",
    "\n",
    "state_func(test_x, test_h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_fff = theano.function([x], [inputs, gate_inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip, gi = t_fff(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gate_inputs': array([[[ -5.29192239e-02,   2.16913983e-01,  -9.17408392e-02,\n",
       "           -1.05395392e-01,  -9.94189084e-03,   1.01206064e-01,\n",
       "           -7.95359910e-03,  -3.18370461e-02,   1.50757402e-01,\n",
       "           -8.21331292e-02,   1.11832857e-01,  -1.64771359e-02,\n",
       "            7.88120851e-02,   2.21751899e-01,  -1.71842992e-01,\n",
       "           -9.92188826e-02,   1.22901015e-01,   1.25320122e-01,\n",
       "            1.92447841e-01,   4.13261726e-03,   9.64488536e-02,\n",
       "            4.19272035e-02,   2.82580014e-02,  -5.71969673e-02,\n",
       "           -5.33812083e-02,  -9.12458897e-02,   1.02144144e-01,\n",
       "            5.70465848e-02,  -1.39211770e-02,   8.24904218e-02,\n",
       "           -1.98895410e-02,   2.70863742e-01,   1.31288007e-01,\n",
       "            2.06264153e-01,   1.51097760e-01,   2.27982644e-02,\n",
       "            1.48788616e-01,  -9.06664729e-02,  -6.25096262e-04,\n",
       "           -5.09598330e-02,  -8.32446665e-03,  -1.63697079e-03,\n",
       "            6.87555298e-02,   1.24091715e-01,  -4.64278162e-02,\n",
       "           -8.41709822e-02,   1.99371919e-01,  -3.79562601e-02,\n",
       "           -1.68697312e-02,  -2.40223613e-02,   1.68625675e-02,\n",
       "            3.65660898e-02,   3.64469811e-02,   1.51907340e-01,\n",
       "            3.03587876e-02,   1.32756487e-01,  -7.92027339e-02,\n",
       "            2.10254192e-02,   2.03587711e-01,   1.43691897e-01,\n",
       "            1.23562232e-01,   9.81250107e-02,   1.83637932e-01,\n",
       "           -2.56508552e-02,   2.81745307e-02,  -5.81748746e-02,\n",
       "            6.21505156e-02,  -5.60520217e-03,   7.04918653e-02,\n",
       "            1.53773442e-01,   1.06657729e-01,   1.72373578e-02,\n",
       "            1.17966004e-01,   5.58492132e-02,   4.43032533e-02,\n",
       "           -1.53730065e-03,   1.33489072e-01,   1.12834752e-01,\n",
       "           -5.78508116e-02,  -1.27551258e-01,  -6.57528043e-02,\n",
       "           -4.71563898e-02,  -1.09237693e-01,   1.13856673e-01,\n",
       "            1.19362883e-01,  -3.32250670e-02,   3.37895975e-02,\n",
       "            2.02296898e-02,  -6.03639446e-02,   8.25738981e-02,\n",
       "            1.03128944e-02,   2.96835732e-02,  -6.56855032e-02,\n",
       "           -1.87769048e-02,   1.18876025e-01,  -2.76999511e-02,\n",
       "            7.43958727e-03,  -2.70773247e-02,   9.83358473e-02,\n",
       "            2.49828696e-02,   2.25095063e-01,   2.40638815e-02,\n",
       "            1.94046035e-01,   1.53268501e-01,   1.50764510e-02,\n",
       "            5.20138666e-02,  -2.54338056e-01,   1.92088876e-02,\n",
       "           -9.01141539e-02,   1.28290104e-02,   1.23059332e-01,\n",
       "           -5.19829467e-02,   1.73625164e-02,   4.82056290e-02,\n",
       "            6.57397285e-02,  -1.32697597e-01,   3.51126790e-02,\n",
       "           -1.28410354e-01,  -1.89337179e-01,   7.14501590e-02,\n",
       "           -4.49448116e-02,   2.91866064e-02,  -4.55235392e-02,\n",
       "            1.60878282e-02,   1.92357842e-02,   7.20753800e-03,\n",
       "           -1.79417700e-01,   1.20070472e-01,   3.44009548e-02,\n",
       "            3.50334831e-02,   6.45014197e-02,   2.23475546e-02,\n",
       "           -2.85367761e-02,  -2.29034908e-02,   2.54838914e-01,\n",
       "           -1.53041288e-01,   1.50982812e-01,   1.83396772e-01,\n",
       "            1.09154619e-02,   2.84200460e-02,   7.10263010e-03,\n",
       "           -6.38797879e-02,   9.02171340e-03,   1.39368866e-02,\n",
       "            2.22828221e-02,  -5.63657796e-03,  -3.39679271e-02,\n",
       "            3.81661672e-03,  -2.79473141e-02,   9.24628079e-02,\n",
       "            5.99913150e-02,  -1.46387517e-01,  -2.87040360e-02,\n",
       "            3.99950862e-01,  -5.89299910e-02,  -7.37386420e-02,\n",
       "            6.71705529e-02,  -3.86834443e-02,   2.30765361e-02,\n",
       "            8.59460831e-02,   1.88068390e-01,  -8.24917182e-02,\n",
       "           -7.03099519e-02,   1.23836458e-01,  -8.56764093e-02,\n",
       "            1.27753109e-01,   1.91577271e-01,  -3.50661501e-02,\n",
       "           -1.06103934e-01,   1.93942934e-01,   1.86325051e-04,\n",
       "           -5.68049103e-02,   1.14644274e-01,   5.30288704e-02,\n",
       "            4.41851728e-02,   6.93215616e-03,   8.92915949e-03,\n",
       "           -2.18933336e-02,   1.00828677e-01,   6.64661825e-02,\n",
       "            9.39296186e-02,  -1.62120149e-01,   1.27622098e-01,\n",
       "            2.70497173e-01,  -9.15624797e-02,   5.51312752e-02,\n",
       "            1.28955692e-01,   1.37669370e-01,   1.22159600e-01,\n",
       "            2.22082570e-01,   1.27071559e-01,   2.39812568e-01,\n",
       "            9.87707172e-03,  -4.33554612e-02,   7.46226385e-02,\n",
       "           -7.76950568e-02,  -7.12233186e-02,   8.45965594e-02,\n",
       "            1.38926461e-01,   1.16654739e-01]]], dtype=float32),\n",
       " 'inputs': array([[[ 0.29306409,  0.27084696,  0.10864701,  0.11313979, -0.11750109,\n",
       "           0.15690269,  0.17739703, -0.38546547,  0.20972852,  0.12706769,\n",
       "           0.39253923,  0.2439642 , -0.26897931, -0.48812386, -0.11818118,\n",
       "          -0.17953715,  0.07221118,  0.19581372,  0.11569399, -0.25499892,\n",
       "           0.34165081,  0.06059962,  0.08417238, -0.25722173,  0.08516702,\n",
       "          -0.17997894,  0.2796886 ,  0.19842789, -0.21638456,  0.15903938,\n",
       "          -0.48903996, -0.07203972,  0.1214742 , -0.04780127,  0.11648569,\n",
       "           0.21297777, -0.08191013, -0.17176265,  0.37724453, -0.08008407,\n",
       "           0.00074494, -0.13026541,  0.08187741, -0.33435306,  0.06105835,\n",
       "           0.19660632,  0.17048985,  0.10414649,  0.1322242 ,  0.16685565,\n",
       "           0.27919585,  0.08972463, -0.10327766, -0.03647786, -0.00784432,\n",
       "          -0.24747014, -0.02894456, -0.00636153,  0.05108041, -0.26948085,\n",
       "          -0.21106195,  0.04855231,  0.01704867,  0.41297466,  0.04481814,\n",
       "           0.08883289,  0.1185179 ,  0.00864755,  0.46234596,  0.32330474,\n",
       "          -0.14674163, -0.24412999, -0.12700826,  0.30956891,  0.1818265 ,\n",
       "           0.06981221,  0.18760699, -0.01691592, -0.15782955, -0.11428092,\n",
       "          -0.08277078,  0.18503693, -0.24204257, -0.15314282,  0.11203273,\n",
       "          -0.09737927,  0.05391427, -0.27755383,  0.04733996, -0.51420456,\n",
       "           0.12037471,  0.11408201, -0.07361674, -0.03340111, -0.28097975,\n",
       "          -0.07199112, -0.08510125, -0.4183335 , -0.02431189,  0.10092518]]], dtype=float32)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_fff(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 52)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ff(test_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_h0 = test_rnnlm.transition.parameters[2].get_value()[None, :]\n",
    "\n",
    "test_x.shape\n",
    "# test_f(test_x, test_h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from blocks tests\n",
    "def test_one_step(self):\n",
    "    h0 = tensor.matrix('h0')\n",
    "    x = tensor.matrix('x')\n",
    "    gi = tensor.matrix('gi')\n",
    "    h1 = self.gated.apply(x, gi, h0, iterate=False)\n",
    "    next_h = theano.function(inputs=[h0, x, gi], outputs=[h1])\n",
    "\n",
    "    h0_val = 0.1 * numpy.array([[1, 1, 0], [0, 1, 1]],\n",
    "                               dtype=theano.config.floatX)\n",
    "    x_val = 0.1 * numpy.array([[1, 2, 3], [4, 5, 6]],\n",
    "                              dtype=theano.config.floatX)\n",
    "    zi_val = (h0_val + x_val) / 2\n",
    "    ri_val = -x_val\n",
    "    W_val = 2 * numpy.ones((3, 3), dtype=theano.config.floatX)\n",
    "\n",
    "    z_val = numpy.tanh(h0_val.dot(W_val) + zi_val)\n",
    "    r_val = numpy.tanh(h0_val.dot(W_val) + ri_val)\n",
    "    h1_val = (z_val * numpy.tanh((r_val * h0_val).dot(W_val) + x_val) +\n",
    "              (1 - z_val) * h0_val)\n",
    "    assert_allclose(\n",
    "        h1_val, next_h(h0_val, x_val, numpy.hstack([zi_val, ri_val]))[0],\n",
    "        rtol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WORKING: implement sampling from the model \n",
    "# outputs of self.sampling_fn:\n",
    "_1, outputs, _2, _3, costs = (self.sampling_fn(inp[None, :]))\n",
    "outputs = outputs.flatten()\n",
    "costs = costs.T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_sample = test_rnnlm.sample(x, sample_length=10)\n",
    "\n",
    "sample = theano.function([x], model_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
